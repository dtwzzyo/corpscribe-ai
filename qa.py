# <<< Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð• Ð—Ð”Ð•Ð¡Ð¬: Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ollama Ð¸Ð· Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð°
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA


PC_IP_ADDRESS = "http://192.168.0.15:11434"

DB_PATH = "./chroma_db"
MODEL_NAME = "mistral"




def main():
    print("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð’Ð¾Ð¿Ñ€Ð¾Ñ-ÐžÑ‚Ð²ÐµÑ‚...")

    llm = Ollama(
        base_url=PC_IP_ADDRESS,
        model=MODEL_NAME
    )
    print(f"âœ… Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ '{MODEL_NAME}' Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾.")


    embeddings = OllamaEmbeddings(model=MODEL_NAME, base_url=PC_IP_ADDRESS)


    vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    print("âœ… Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð±Ð°Ð·Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°.")


    retriever = vectorstore.as_retriever(search_type="mmr")
    print("âœ… Ð ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€ Ð³Ð¾Ñ‚Ð¾Ð² Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ðµ.")

    # 5. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑˆÐ°Ð±Ð»Ð¾Ð½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°.
    #    Ð­Ñ‚Ð¾ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ LLM, ÐºÐ°Ðº Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ.
    #    {context} - ÑÑŽÐ´Ð° LangChain Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð¸Ñ‚ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð² Ð±Ð°Ð·Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹.
    #    {question} - ÑÑŽÐ´Ð° LangChain Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð¸Ñ‚ Ð½Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.
    template = """
    Ð¢Ñ‹ â€” ÑƒÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð²ÐµÐ¶Ð»Ð¸Ð²Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸.
    Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð¾ÑÐ½Ð¾Ð²Ñ‹Ð²Ð°ÑÑÑŒ Ð˜Ð¡ÐšÐ›Ð®Ð§Ð˜Ð¢Ð•Ð›Ð¬ÐÐž Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ.
    Ð•ÑÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾ ÑÐºÐ°Ð¶Ð¸, Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¸Ð¼ÐµÑŽÑ‰Ð¸Ñ…ÑÑ Ð´Ð°Ð½Ð½Ñ‹Ñ….
    ÐÐµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð¾Ñ‚ ÑÐµÐ±Ñ.

    ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:
    {context}

    Ð’Ð¾Ð¿Ñ€Ð¾Ñ:
    {question}

    ÐžÑ‚Ð²ÐµÑ‚:
    """
    prompt = PromptTemplate.from_template(template)

    # 6. Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð²Ð¼ÐµÑÑ‚Ðµ Ð² ÐµÐ´Ð¸Ð½ÑƒÑŽ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÑƒ (QA Chain)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    print("âœ… Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ð¿Ñ€Ð¸ÐµÐ¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð².\n" + "="*50)


    question = "Ð“Ð´Ðµ Ð²ÐµÐ´ÐµÑ‚ÑÑ Ð²ÑÑ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ?" 

    print(f"ðŸ’¬ Ð’Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ: {question}\n")
    print("ðŸ¤” Ð”ÑƒÐ¼Ð°ÑŽ Ð½Ð°Ð´ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼...")

    result = qa_chain({"query": question})
    answer = result["result"]
    source_documents = result["source_documents"]

    print("\n" + "="*50)
    print("ðŸ¤– ÐžÑ‚Ð²ÐµÑ‚:")
    print(answer)
    print("\n" + "="*50)

    if source_documents:
        print("ðŸ“š Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:")
        for doc in source_documents:
            print(f"  - {doc.metadata['source']} (Ñ‡Ð°ÑÑ‚ÑŒ Ñ‚ÐµÐºÑÑ‚Ð°: \"{doc.page_content[:100]}...\")")

if __name__ == "__main__":
    main()
